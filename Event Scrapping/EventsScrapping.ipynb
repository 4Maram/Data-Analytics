{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca194994-be3f-41d7-a4b8-d1d94d98e843",
   "metadata": {
    "tags": []
   },
   "source": [
    "# sending GET request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f45654-e28a-4a76-ae0a-3999efe1883f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b98a44c-50c5-4e87-8ee8-805e7f4538df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://www.eyeofriyadh.com/events/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24902e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974d22b-b837-41ff-9dea-08ee5cb3172d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text , 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e8613-86f6-4066-97ad-99dc84601ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# soup.find('div', class_ = 'content_left').text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12b2b7-3364-48ce-ab78-f6577f7bd0dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# investigate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac46e52-0bd8-488f-b70f-6cfe2516c070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715f6c2-b95b-422c-b01e-4a3d449fa5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41796223-5a82-4d05-8c98-2e8b44d3df26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612eb74-2e6b-4b0f-aaf8-d0613c8cf024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78a48d-69e4-46b0-aef6-8b2e0f8691ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scrap the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69601839-66c5-4214-8fe9-33fb8f2a529f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n"
     ]
    }
   ],
   "source": [
    "### 1st Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 1\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dfbb54f-c286-45ec-ab36-88bc25457507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27 - 29 Jan, 2025\\n\\nReal Estate Future Forum\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23 Jul - 17 Aug, 2024\\n\\nThird International C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28 Jul - 29 Aug, 2024\\n\\nPrince Faisal bin Fah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4 - 8 Aug, 2024\\n\\nE-commerce Week for SMEs by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12 - 15 Aug, 2024\\n\\nSaudi Food Expo \\n\\nRiyad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20 - 22 Aug, 2024\\n\\nAesthetic Medical Forum\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24 - 25 Aug, 2024\\n\\nNew Global Sport Conferen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27 - 29 Aug, 2024\\n\\n 2nd Saudi Fashiontex Exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2 - 4 Sep, 2024\\n\\nSaudi Wood Expo 2024\\n\\nRiy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2 - 4 Sep, 2024\\n\\nSaudi Warehousing &amp; Logisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3 - 5 Sep, 2024\\n\\n24 Fintech Conference\\n\\nRi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>« Previous 1 2 3 4 5 6 7 8 9 ... 20 Next »</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0                                                    \n",
       "1                                                    \n",
       "2                                                    \n",
       "3                                                    \n",
       "4                                                    \n",
       "5   27 - 29 Jan, 2025\\n\\nReal Estate Future Forum\\...\n",
       "6                                                    \n",
       "7   23 Jul - 17 Aug, 2024\\n\\nThird International C...\n",
       "8                                                    \n",
       "9   28 Jul - 29 Aug, 2024\\n\\nPrince Faisal bin Fah...\n",
       "10                                                   \n",
       "11  4 - 8 Aug, 2024\\n\\nE-commerce Week for SMEs by...\n",
       "12                                                   \n",
       "13  12 - 15 Aug, 2024\\n\\nSaudi Food Expo \\n\\nRiyad...\n",
       "14                                                   \n",
       "15  20 - 22 Aug, 2024\\n\\nAesthetic Medical Forum\\n...\n",
       "16                                                   \n",
       "17  24 - 25 Aug, 2024\\n\\nNew Global Sport Conferen...\n",
       "18                                                   \n",
       "19  27 - 29 Aug, 2024\\n\\n 2nd Saudi Fashiontex Exp...\n",
       "20                                                   \n",
       "21  2 - 4 Sep, 2024\\n\\nSaudi Wood Expo 2024\\n\\nRiy...\n",
       "22                                                   \n",
       "23  2 - 4 Sep, 2024\\n\\nSaudi Warehousing & Logisti...\n",
       "24                                                   \n",
       "25  3 - 5 Sep, 2024\\n\\n24 Fintech Conference\\n\\nRi...\n",
       "26                                                   \n",
       "27                                                   \n",
       "28                                                   \n",
       "29         « Previous 1 2 3 4 5 6 7 8 9 ... 20 Next »\n",
       "30                                                   \n",
       "31                                                   \n",
       "32                                                   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=ls)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902be0f-a704-451d-a5e6-aae8f1be34fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6eba3e-e54f-4b4f-98be-56197b73b4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af306c2a-3273-4e79-90c7-92eb49c5d439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[0].str.split('\\\\n', n=8 , expand=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced3f075-a941-4aa8-9596-d0ce45da7f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns = {0: 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb6be73b-df2a-4e1b-8a76-b5ddae48f744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns = {2: 'Event'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ce45d5-bb39-4886-98de-ab8ff21ea8ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns = {4: 'Location'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ced747-7e7c-4e6f-abcd-80d1310f37c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns = {7: 'Details'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cc744-3d09-42b6-b183-815cc0109500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64c9ca4f-36dc-4206-9447-fd176d881d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=[1,3,5,6,8], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a772695a-b8e5-4035-bf8e-76ed170524a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['Location','Category']]=df['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31ef0300-1e50-4293-885a-97377247a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n"
     ]
    }
   ],
   "source": [
    "### 2nd Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 2\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81b64dc9-0336-4465-a577-1dc03e0c33fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data=ls)\n",
    "df2\n",
    "df2.drop_duplicates(inplace=True)\n",
    "df2 = df2[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df2.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df2.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df2.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df2.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df2.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df2[['Location','Category']]=df2['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "325ce112-127c-478f-bfd2-65854d759fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Event</th>\n",
       "      <th>Location</th>\n",
       "      <th>Details</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8 - 10 Sep, 2024</td>\n",
       "      <td>The International facility Management Forum an...</td>\n",
       "      <td>Fairmont Riyadh , Riyadh</td>\n",
       "      <td>The International facility  Management Forum a...</td>\n",
       "      <td>Conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9 - 11 Sep, 2024</td>\n",
       "      <td>International Manufacturing Congress 2024</td>\n",
       "      <td>Riyadh International Convention &amp; Exhibition C...</td>\n",
       "      <td>The International Manufacturing Congress will ...</td>\n",
       "      <td>Conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10 - 12 Sep, 2024</td>\n",
       "      <td>Saudi Sport Show 2024</td>\n",
       "      <td>The Arena Riyadh Venue for Exhibitions , Riyadh</td>\n",
       "      <td>Saudi Sport Show is a world leading event for ...</td>\n",
       "      <td>Exhibition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10 - 12 Sep, 2024</td>\n",
       "      <td>Global AI Summit</td>\n",
       "      <td>King abdulaziz international conferance center...</td>\n",
       "      <td>Under the patronage of His Royal Highness Prin...</td>\n",
       "      <td>Summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10 - 12 Sep, 2024</td>\n",
       "      <td>MetaGate Summit</td>\n",
       "      <td>Riyadh</td>\n",
       "      <td>MetaGate Summit is set in Riyadh, Saudi Arabia...</td>\n",
       "      <td>Summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11 - 12 Sep, 2024</td>\n",
       "      <td>The Saudi Event Show</td>\n",
       "      <td>Mandarin Oriental Al Faisaliah Hotel , Riyadh</td>\n",
       "      <td>The Saudi Event Show is returning to Riyadh fo...</td>\n",
       "      <td>Exhibition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16 - 19 Sep, 2024</td>\n",
       "      <td>Foodex Saudi 2024</td>\n",
       "      <td>Riyadh International Convention &amp; Exhibition C...</td>\n",
       "      <td>Saudi Arabia’s Leading International Food &amp; Dr...</td>\n",
       "      <td>Exhibition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16 - 17 Sep, 2024</td>\n",
       "      <td>The Building Envelope Design and Insulation Co...</td>\n",
       "      <td>Jeddah, Saudi Arabia , Jeddah</td>\n",
       "      <td>Event Overview: The Building Envelope Design a...</td>\n",
       "      <td>Conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16 - 17 Sep, 2024</td>\n",
       "      <td>MEP International Conference (MEPIC) 2024</td>\n",
       "      <td>Jeddah, Saudi Arabia , Jeddah</td>\n",
       "      <td>Event Overview: MEP International Conference (...</td>\n",
       "      <td>Conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17 - 19 Sep, 2024</td>\n",
       "      <td>Lighting Design &amp; Technology Expo</td>\n",
       "      <td>RIYADH FRONT EXHIBITION &amp; CONFERENCE CENTER , ...</td>\n",
       "      <td>The growing trend of smart homes and the devel...</td>\n",
       "      <td>Exhibition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>« Previous 1 2 3 4 5 6 7 8 9 ... 20 Next »</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Date  \\\n",
       "0                                                \n",
       "5                             8 - 10 Sep, 2024   \n",
       "7                             9 - 11 Sep, 2024   \n",
       "9                            10 - 12 Sep, 2024   \n",
       "11                           10 - 12 Sep, 2024   \n",
       "13                           10 - 12 Sep, 2024   \n",
       "15                           11 - 12 Sep, 2024   \n",
       "17                           16 - 19 Sep, 2024   \n",
       "19                           16 - 17 Sep, 2024   \n",
       "21                           16 - 17 Sep, 2024   \n",
       "23                           17 - 19 Sep, 2024   \n",
       "27  « Previous 1 2 3 4 5 6 7 8 9 ... 20 Next »   \n",
       "\n",
       "                                                Event  \\\n",
       "0                                                None   \n",
       "5   The International facility Management Forum an...   \n",
       "7           International Manufacturing Congress 2024   \n",
       "9                               Saudi Sport Show 2024   \n",
       "11                                   Global AI Summit   \n",
       "13                                    MetaGate Summit   \n",
       "15                               The Saudi Event Show   \n",
       "17                                  Foodex Saudi 2024   \n",
       "19  The Building Envelope Design and Insulation Co...   \n",
       "21          MEP International Conference (MEPIC) 2024   \n",
       "23                 Lighting Design & Technology Expo    \n",
       "27                                               None   \n",
       "\n",
       "                                             Location  \\\n",
       "0                                                None   \n",
       "5                           Fairmont Riyadh , Riyadh    \n",
       "7   Riyadh International Convention & Exhibition C...   \n",
       "9    The Arena Riyadh Venue for Exhibitions , Riyadh    \n",
       "11  King abdulaziz international conferance center...   \n",
       "13                                            Riyadh    \n",
       "15     Mandarin Oriental Al Faisaliah Hotel , Riyadh    \n",
       "17  Riyadh International Convention & Exhibition C...   \n",
       "19                     Jeddah, Saudi Arabia , Jeddah    \n",
       "21                     Jeddah, Saudi Arabia , Jeddah    \n",
       "23  RIYADH FRONT EXHIBITION & CONFERENCE CENTER , ...   \n",
       "27                                               None   \n",
       "\n",
       "                                              Details     Category  \n",
       "0                                                None         None  \n",
       "5   The International facility  Management Forum a...   Conference  \n",
       "7   The International Manufacturing Congress will ...   Conference  \n",
       "9   Saudi Sport Show is a world leading event for ...   Exhibition  \n",
       "11  Under the patronage of His Royal Highness Prin...       Summit  \n",
       "13  MetaGate Summit is set in Riyadh, Saudi Arabia...       Summit  \n",
       "15  The Saudi Event Show is returning to Riyadh fo...   Exhibition  \n",
       "17  Saudi Arabia’s Leading International Food & Dr...   Exhibition  \n",
       "19  Event Overview: The Building Envelope Design a...   Conference  \n",
       "21  Event Overview: MEP International Conference (...   Conference  \n",
       "23  The growing trend of smart homes and the devel...   Exhibition  \n",
       "27                                               None         None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af153bf2-8a51-40b2-a97b-cabe2fee740d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n"
     ]
    }
   ],
   "source": [
    "### 3rd Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 3\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df3 = pd.DataFrame(data=ls)\n",
    "df3\n",
    "df3.drop_duplicates(inplace=True)\n",
    "df3 = df3[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df3.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df3.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df3.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df3.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df3.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df3[['Location','Category']]=df3['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e2112b9-8f56-4fb2-9883-0ad35cfa8817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n"
     ]
    }
   ],
   "source": [
    "### 4th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 4\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df4 = pd.DataFrame(data=ls)\n",
    "df4\n",
    "df4.drop_duplicates(inplace=True)\n",
    "df4 = df4[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df4.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df4.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df4.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df4.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df4.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df4[['Location','Category']]=df4['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "615b9d27-6802-43b7-91c5-234113da6be0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n"
     ]
    }
   ],
   "source": [
    "### 5th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 5\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df5 = pd.DataFrame(data=ls)\n",
    "df5\n",
    "df5.drop_duplicates(inplace=True)\n",
    "df5 = df5[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df5.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df5.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df5.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df5.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df5.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df5[['Location','Category']]=df5['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49f1ddad-f6f6-46b4-a84c-5baff0dca8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n"
     ]
    }
   ],
   "source": [
    "### 6th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 6\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df6 = pd.DataFrame(data=ls)\n",
    "df6\n",
    "df6.drop_duplicates(inplace=True)\n",
    "df6 = df6[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df6.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df6.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df6.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df6.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df6.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df6[['Location','Category']]=df6['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff857b24-1a8a-49fc-8621-10cb74f20e47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n"
     ]
    }
   ],
   "source": [
    "### 7th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 7\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df7 = pd.DataFrame(data=ls)\n",
    "df7\n",
    "df7.drop_duplicates(inplace=True)\n",
    "df7 = df7[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df7.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df7.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df7.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df7.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df7.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df7[['Location','Category']]=df7['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60cfa281-27f5-4f57-8e9a-ae843025e4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n"
     ]
    }
   ],
   "source": [
    "### 8th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 8\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df8 = pd.DataFrame(data=ls)\n",
    "df8\n",
    "df8.drop_duplicates(inplace=True)\n",
    "df8 = df8[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df8.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df8.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df8.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df8.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df8.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df8[['Location','Category']]=df8['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c15f828-4dd0-48ef-a4ac-1610be75fb44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n"
     ]
    }
   ],
   "source": [
    "### 9th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 9\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df9 = pd.DataFrame(data=ls)\n",
    "df9\n",
    "df9.drop_duplicates(inplace=True)\n",
    "df9 = df9[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df9.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df9.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df9.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df9.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df9.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df9[['Location','Category']]=df9['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3665bd64-a3b2-4924-a8e1-be8a95576640",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n"
     ]
    }
   ],
   "source": [
    "### 10th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 10\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df10 = pd.DataFrame(data=ls)\n",
    "df10\n",
    "df10.drop_duplicates(inplace=True)\n",
    "df10 = df10[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df10.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df10.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df10.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df10.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df10.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df10[['Location','Category']]=df10['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a458d59-be61-45bd-9713-b10fb35d8f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n"
     ]
    }
   ],
   "source": [
    "### 11th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 11\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df11 = pd.DataFrame(data=ls)\n",
    "df11\n",
    "df11.drop_duplicates(inplace=True)\n",
    "df11 = df11[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df11.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df11.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df11.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df11.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df11.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df11[['Location','Category']]=df11['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e11f4321-a763-4631-a65c-3baf0f07ccd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n"
     ]
    }
   ],
   "source": [
    "### 12th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 12\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df12 = pd.DataFrame(data=ls)\n",
    "df12\n",
    "df12.drop_duplicates(inplace=True)\n",
    "df12 = df12[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df12.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df12.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df12.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df12.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df12.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df12[['Location','Category']]=df12['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e8ca318-0a47-4ee5-9547-4a1e4845cdf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n"
     ]
    }
   ],
   "source": [
    "### 13th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 13\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df13 = pd.DataFrame(data=ls)\n",
    "df13\n",
    "df13.drop_duplicates(inplace=True)\n",
    "df13 = df13[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df13.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df13.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df13.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df13.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df13.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df13[['Location','Category']]=df13['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87a80845-8af3-4535-af44-f9b9e8be888f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Page 14\n"
     ]
    }
   ],
   "source": [
    "### 14th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 14\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df14 = pd.DataFrame(data=ls)\n",
    "df14\n",
    "df14.drop_duplicates(inplace=True)\n",
    "df14 = df14[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df14.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df14.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df14.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df14.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df14.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df14[['Location','Category']]=df14['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b484934-8905-448d-a6d3-4abad0ef0e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Page 14\n",
      "Page 15\n"
     ]
    }
   ],
   "source": [
    "### 15th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 15\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df15 = pd.DataFrame(data=ls)\n",
    "df15\n",
    "df15.drop_duplicates(inplace=True)\n",
    "df15 = df15[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df15.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df15.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df15.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df15.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df15.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df15[['Location','Category']]=df15['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc346ca2-1a73-4988-a1ec-8f3d9684d720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Page 14\n",
      "Page 15\n",
      "Page 16\n"
     ]
    }
   ],
   "source": [
    "### 16th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 16\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df16 = pd.DataFrame(data=ls)\n",
    "df16\n",
    "df16.drop_duplicates(inplace=True)\n",
    "df16 = df16[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df16.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df16.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df16.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df16.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df16.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df16[['Location','Category']]=df16['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5686ceb7-f4c0-4ecc-9ab0-540b2135fad7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Page 14\n",
      "Page 15\n",
      "Page 16\n",
      "Page 17\n"
     ]
    }
   ],
   "source": [
    "### 17th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 17\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df17 = pd.DataFrame(data=ls)\n",
    "df17\n",
    "df17.drop_duplicates(inplace=True)\n",
    "df17 = df17[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df17.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df17.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df17.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df17.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df17.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df17[['Location','Category']]=df17['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "916a30fb-33a4-428d-8e2d-2fdef6242d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Page 14\n",
      "Page 15\n",
      "Page 16\n",
      "Page 17\n",
      "Page 18\n"
     ]
    }
   ],
   "source": [
    "### 18th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 18\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df18 = pd.DataFrame(data=ls)\n",
    "df18\n",
    "df18.drop_duplicates(inplace=True)\n",
    "df18 = df18[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df18.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df18.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df18.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df18.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df18.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df18[['Location','Category']]=df18['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c277da8b-0f77-439f-8d06-14b6355e66f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Page 14\n",
      "Page 15\n",
      "Page 16\n",
      "Page 17\n",
      "Page 18\n",
      "Page 19\n"
     ]
    }
   ],
   "source": [
    "### 19th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 19\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df19 = pd.DataFrame(data=ls)\n",
    "df19\n",
    "df19.drop_duplicates(inplace=True)\n",
    "df19 = df19[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df19.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df19.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df19.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df19.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df19.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df19[['Location','Category']]=df19['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "569d6585-f50b-4501-8e02-658c15c45c73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Page 14\n",
      "Page 15\n",
      "Page 16\n",
      "Page 17\n",
      "Page 18\n",
      "Page 19\n",
      "Page 20\n"
     ]
    }
   ],
   "source": [
    "### 20th Page :::\n",
    "# Function to scrape data from all pages\n",
    "num_pages = 20\n",
    "for page in range(1, num_pages + 1):\n",
    "    # URL of the website to scrape\n",
    "    url = f'https://www.eyeofriyadh.com/events/index.php?page={page}&ipp=10'\n",
    "    ls = []\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the HTML content\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all event divs\n",
    "        event_divs = soup.find_all('div', class_='content_left')\n",
    "        print(f\"Page {page}\")\n",
    "        \n",
    "        # Extract event details for each event div and append to the list\n",
    "        for event_div in event_divs: \n",
    "            e = event_div\n",
    "            a = [e.text.strip() for e in event_div]\n",
    "            for _ in a:\n",
    "                ls.append(_)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch the content. Status code:', response.status_code)\n",
    "        \n",
    "df20 = pd.DataFrame(data=ls)\n",
    "df20\n",
    "df20.drop_duplicates(inplace=True)\n",
    "df20 = df20[0].str.split('\\\\n', n=8 , expand=True )\n",
    "df20.rename(columns = {0: 'Date'}, inplace=True)\n",
    "df20.rename(columns = {2: 'Event'}, inplace=True)\n",
    "df20.rename(columns = {4: 'Location'}, inplace=True)\n",
    "df20.rename(columns = {7: 'Details'}, inplace=True)\n",
    "df20.drop(columns=[1,3,5,6,8], inplace = True)\n",
    "df20[['Location','Category']]=df20['Location'].str.split(\"|\" ,expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85a458-95e1-47f1-a77a-9af728e6f2eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cfe4738-276b-472e-ab3e-587cd92a18b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b217b7b-8087-42b8-ae34-4155220b9384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames = [df,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20\n",
    "         ]\n",
    "d = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7b6043b-7f58-4f3e-8ac3-5970a50dffa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "588d3dc9-cf62-46a2-a81a-207b1790ffdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "614cb457-1b82-4bb5-a887-5f2c359d2310",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 237 entries, 0 to 19\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Date      237 non-null    object\n",
      " 1   Event     197 non-null    object\n",
      " 2   Location  197 non-null    object\n",
      " 3   Details   197 non-null    object\n",
      " 4   Category  197 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 11.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d79b339-1107-4bb4-b4b4-17e933bc1c24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee2252b8-1d2b-4411-927e-0945b38bf42c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dd.drop_duplicates(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79050d17-1efe-4100-932f-eab2c1b61295",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Event</th>\n",
       "      <th>Location</th>\n",
       "      <th>Details</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27 - 29 Jan, 2025</td>\n",
       "      <td>Real Estate Future Forum</td>\n",
       "      <td>Four Seasons Hotel , Riyadh</td>\n",
       "      <td>The forum makes it possible for significant na...</td>\n",
       "      <td>Forum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23 Jul - 17 Aug, 2024</td>\n",
       "      <td>Third International Conference on the History ...</td>\n",
       "      <td>Virtual , Online</td>\n",
       "      <td>Imam Mohammed Bin Saud Islamic University invi...</td>\n",
       "      <td>Conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28 Jul - 29 Aug, 2024</td>\n",
       "      <td>Prince Faisal bin Fahd Arts Hall Exhibition: S...</td>\n",
       "      <td>Prince Faisal bin Fahd Arts Hall , Riyadh</td>\n",
       "      <td>Misk Art Institute presents the second iterati...</td>\n",
       "      <td>Exhibition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4 - 8 Aug, 2024</td>\n",
       "      <td>E-commerce Week for SMEs by Monshaat</td>\n",
       "      <td>SMEs Support Center, Riyadh , Riyadh</td>\n",
       "      <td>The General Authority for Small and Medium Ent...</td>\n",
       "      <td>Conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25 - 27 Nov, 2024</td>\n",
       "      <td>Qatar International Exhibition for Travel &amp; To...</td>\n",
       "      <td>DECC- Doha, Qatar , Doha</td>\n",
       "      <td>The third edition of Qatar Travel Mart (QTM202...</td>\n",
       "      <td>Exhibition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28 Apr - 1 May, 2025</td>\n",
       "      <td>Build Your House Exhibition - 6th Edition</td>\n",
       "      <td>Qatar National Convention Centre (QNCC) , Doha</td>\n",
       "      <td>Build Your House 2025 (BYH 2025), is the Premi...</td>\n",
       "      <td>Exhibition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13 - 16 Oct, 2024</td>\n",
       "      <td>7th International Maintenance, Reliability, an...</td>\n",
       "      <td>Exhibition World Bahrain, Kingdom of Bahrain ,...</td>\n",
       "      <td>MAINTCON has steadily grown to be one of the l...</td>\n",
       "      <td>Conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3 - 4 Sep, 2024</td>\n",
       "      <td>Africa Oil and Gas Digital Transformation Conf...</td>\n",
       "      <td>Lagos</td>\n",
       "      <td>Event Overview: Africa Oil and Gas Digital Tra...</td>\n",
       "      <td>Conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26 - 28 Nov, 2024</td>\n",
       "      <td>MAPIC 2024</td>\n",
       "      <td>Palais Des Festivals , Cannes</td>\n",
       "      <td>MAPIC is the leading international gathering o...</td>\n",
       "      <td>Exhibition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date                                              Event  \\\n",
       "0                                                                       None   \n",
       "5       27 - 29 Jan, 2025                           Real Estate Future Forum   \n",
       "7   23 Jul - 17 Aug, 2024  Third International Conference on the History ...   \n",
       "9   28 Jul - 29 Aug, 2024  Prince Faisal bin Fahd Arts Hall Exhibition: S...   \n",
       "11        4 - 8 Aug, 2024               E-commerce Week for SMEs by Monshaat   \n",
       "..                    ...                                                ...   \n",
       "7       25 - 27 Nov, 2024  Qatar International Exhibition for Travel & To...   \n",
       "9    28 Apr - 1 May, 2025         Build Your House Exhibition - 6th Edition    \n",
       "11      13 - 16 Oct, 2024  7th International Maintenance, Reliability, an...   \n",
       "13        3 - 4 Sep, 2024  Africa Oil and Gas Digital Transformation Conf...   \n",
       "15      26 - 28 Nov, 2024                                         MAPIC 2024   \n",
       "\n",
       "                                             Location  \\\n",
       "0                                                None   \n",
       "5                        Four Seasons Hotel , Riyadh    \n",
       "7                                   Virtual , Online    \n",
       "9          Prince Faisal bin Fahd Arts Hall , Riyadh    \n",
       "11              SMEs Support Center, Riyadh , Riyadh    \n",
       "..                                                ...   \n",
       "7                           DECC- Doha, Qatar , Doha    \n",
       "9     Qatar National Convention Centre (QNCC) , Doha    \n",
       "11  Exhibition World Bahrain, Kingdom of Bahrain ,...   \n",
       "13                                             Lagos    \n",
       "15                     Palais Des Festivals , Cannes    \n",
       "\n",
       "                                              Details     Category  \n",
       "0                                                None         None  \n",
       "5   The forum makes it possible for significant na...        Forum  \n",
       "7   Imam Mohammed Bin Saud Islamic University invi...   Conference  \n",
       "9   Misk Art Institute presents the second iterati...   Exhibition  \n",
       "11  The General Authority for Small and Medium Ent...   Conference  \n",
       "..                                                ...          ...  \n",
       "7   The third edition of Qatar Travel Mart (QTM202...   Exhibition  \n",
       "9   Build Your House 2025 (BYH 2025), is the Premi...   Exhibition  \n",
       "11  MAINTCON has steadily grown to be one of the l...   Conference  \n",
       "13  Event Overview: Africa Oil and Gas Digital Tra...   Conference  \n",
       "15  MAPIC is the leading international gathering o...   Exhibition  \n",
       "\n",
       "[197 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '«'\n",
    "a = dd[~dd['Date'].str.startswith(text)]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd1976-15a5-4eb6-b76c-6a603ecf2763",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract Data into .csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "776979fb-f14c-43f8-894e-686c413387b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "data= a.to_csv('events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9599e1-673d-4c51-8082-456b057407a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf263f72-591e-4cd1-9ba4-c20f9edcc21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35ba29-e260-4032-85e6-59b2995e5422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fce943-f116-4a69-bf74-16d75d5eefde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b310f88-e6fb-4ff3-9057-0f3d0710ca88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d50909-4c8c-4290-a296-c115c49e36cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a06170e-6065-4a01-b183-85c46b9c60ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d380f-dbb0-4f4f-af0e-2c5ba93f62f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7243a-dae3-447f-8df3-b24ad0da756c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89404d8d-2f1d-4473-b42d-4ea33da49c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae7297-da4a-4fbb-aba1-efebe61c3dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5563b-ed59-4885-b450-fe9404864990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634cb43-224b-4250-a5f3-5e6fb98c80ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
